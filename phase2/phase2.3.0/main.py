
import streamlit as st
from janome.tokenizer import Tokenizer
import os
import re
from transformers import pipeline
import torch

TEXT_DIR = "text"

# å“è©ã”ã¨ã®è‰²æŒ‡å®š
pos_colors = {
    "åè©": "#ffadad",
    "å‹•è©": "#ffd6a5",
    "å½¢å®¹è©": "#caffbf",
    "å‰¯è©": "#9bf6ff",
    "åŠ©è©": "#bdb2ff",
    "è¨˜å·": "#ffffff",
    "ãã®ä»–": "#d3d3d3"
}

tokenizer = Tokenizer()

@st.cache_resource
def load_classifier():
    local_path = "/Users/nknt/.cache/huggingface/hub/models--MoritzLaurer--mDeBERTa-v3-base-xnli-multilingual-nli-2mil7"
    if not os.path.exists(local_path):
        classifier = pipeline(
            "zero-shot-classification",
            model="MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7",
            device=-1
        )
    else:
        classifier = pipeline(
            "zero-shot-classification",
            model=local_path,
            device=-1
        )
    return classifier



# # ã‚¿ã‚¹ã‚¯æŠ½å‡ºç”¨ã®è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
# @st.cache_resource
# def load_classifier():
#     # CPUç’°å¢ƒã§ã‚‚å‹•ä½œã™ã‚‹æ—¥æœ¬èªBERTãƒ™ãƒ¼ã‚¹ã®è»½é‡ãƒ¢ãƒ‡ãƒ«
#     # tohoku-nlp/bert-base-japanese-v3 ã¯æ¯”è¼ƒçš„è»½é‡ã§æ€§èƒ½ã‚‚è‰¯ã„
#     classifier = pipeline(
#         "zero-shot-classification",
#         model="MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7",
#         device=-1  # CPUä½¿ç”¨
#     )
#     return classifier


def split_sentences(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡å˜ä½ã«åˆ†å‰²"""
    # æ—¥æœ¬èªã®æ–‡æœ«è¡¨ç¾ã§åˆ†å‰²
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
    # ç©ºæ–‡å­—åˆ—ã‚’é™¤å»
    sentences = [s.strip() for s in sentences if s.strip()]
    return sentences

def extract_tasks(text, classifier):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŒ‡ç¤ºãƒ»ä¾é ¼æ–‡ã‚’æŠ½å‡º"""
    sentences = split_sentences(text)
    tasks = []
    
    # æŒ‡ç¤ºãƒ»ä¾é ¼ã‚’ç¤ºã™ãƒ©ãƒ™ãƒ«
    candidate_labels = ["æŒ‡ç¤º", "ä¾é ¼", "è¦æ±‚", "ã‚¿ã‚¹ã‚¯", "ãã®ä»–"]
    
    for sentence in sentences:
        if len(sentence) < 5:  # çŸ­ã™ãã‚‹æ–‡ã¯é™¤å¤–
            continue
            
        # ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆåˆ†é¡
        result = classifier(
            sentence,
            candidate_labels=candidate_labels,
            hypothesis_template="ã“ã®æ–‡ã¯{}ã§ã™ã€‚"
        )
        
        # ã‚¹ã‚³ã‚¢ãŒé«˜ãã€æŒ‡ç¤ºãƒ»ä¾é ¼ç³»ã®ãƒ©ãƒ™ãƒ«ãŒä¸Šä½ã®å ´åˆ
        if result['labels'][0] in ["æŒ‡ç¤º", "ä¾é ¼", "è¦æ±‚", "ã‚¿ã‚¹ã‚¯"] and result['scores'][0] > 0.5:
            tasks.append({
                'text': sentence,
                'score': result['scores'][0],
                'label': result['labels'][0]
            })
    
    return tasks

def highlight_pos(text):
    html = ""
    for token in tokenizer.tokenize(text):
        word = token.surface
        pos = token.part_of_speech.split(",")[0]
        color = pos_colors.get(pos, pos_colors["ãã®ä»–"])
        html += f'<span style="background-color:{color}; padding:2px; margin:1px; border-radius:3px;">{word}</span>'
    return html

# åˆæœŸåŒ–
if "last_seen_files" not in st.session_state:
    st.session_state.last_seen_files = set()
if "all_tasks" not in st.session_state:
    st.session_state.all_tasks = {}

st.title("ğŸ“‚ è‡ªå‹•ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºãƒ“ãƒ¥ãƒ¼ã‚¢ with ã‚¿ã‚¹ã‚¯æŠ½å‡º")
st.markdown("### `text/` ãƒ•ã‚©ãƒ«ãƒ€ã«æ–°ã—ã„ `.txt` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿ï¼‰
with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™..."):
    classifier = load_classifier()

# ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
current_files = {f for f in os.listdir(TEXT_DIR) if f.endswith(".txt")}
new_files = current_files - st.session_state.last_seen_files

# æ–°ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°å‡¦ç†
if new_files:
    for file_name in sorted(new_files):
        file_path = os.path.join(TEXT_DIR, file_name)
        st.success(f"âœ… æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º: {file_name}")

        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()

        # ã‚¿ã‚¹ã‚¯æŠ½å‡º
        with st.spinner("ã‚¿ã‚¹ã‚¯ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™..."):
            tasks = extract_tasks(text, classifier)
            if tasks:
                st.session_state.all_tasks[file_name] = tasks

        with st.expander(f"â–¶ {file_name} ã®å†…å®¹"):
            st.text(text)

        st.markdown("#### ğŸ” å“è©ã”ã¨ã«è‰²åˆ†ã‘ã•ã‚ŒãŸè¡¨ç¤º")
        st.markdown(highlight_pos(text), unsafe_allow_html=True)

        # æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’è¡¨ç¤º
        if tasks:
            st.markdown("#### ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒ»æŒ‡ç¤º")
            for i, task in enumerate(tasks):
                col1, col2 = st.columns([4, 1])
                with col1:
                    done = st.checkbox(
                        task['text'], 
                        key=f"{file_name}_task_{i}",
                        help=f"ä¿¡é ¼åº¦: {task['score']:.2f} ({task['label']})"
                    )
                with col2:
                    st.caption(f"ä¿¡é ¼åº¦: {task['score']:.1%}")

    # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å«ã‚ã¦æ›´æ–°
    st.session_state.last_seen_files.update(new_files)
else:
    st.info("ç¾åœ¨ã€æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

# å…¨ä½“ã®ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆè¡¨ç¤º
if st.session_state.all_tasks:
    st.markdown("---")
    st.markdown("### ğŸ“Œ å…¨ä½“ã®ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ")
    for file_name, tasks in st.session_state.all_tasks.items():
        with st.expander(f"ğŸ“„ {file_name} ã®ã‚¿ã‚¹ã‚¯"):
            for i, task in enumerate(tasks):
                st.checkbox(
                    task['text'], 
                    key=f"all_{file_name}_task_{i}",
                    help=f"ä¿¡é ¼åº¦: {task['score']:.2f}"
                )

# è‡ªå‹•ãƒªãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
st.button("ğŸ”„ å†èª­ã¿è¾¼ã¿ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§æ›´æ–°ï¼‰")
